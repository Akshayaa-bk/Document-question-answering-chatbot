{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5XRf_DJz9NLk"
      },
      "outputs": [],
      "source": [
        "!rm -rf /project*\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory structure\n",
        "!mkdir -p /project/utils\n",
        "\n",
        "# Create main files\n",
        "!touch /project/main.py\n",
        "!touch /project/app.py\n",
        "!touch /project/requirements.txt\n",
        "!touch /project/.gitignore\n",
        "!touch /project/.env\n",
        "\n",
        "# Create utils package and utility modules\n",
        "!touch /project/utils/__init__.py\n",
        "!touch /project/utils/document_processor.py\n",
        "!touch /project/utils/embedding_manager.py\n",
        "!touch /project/utils/retrieval_engine.py\n",
        "!touch /project/utils/llm_interface.py\n"
      ],
      "metadata": {
        "id": "y_7zTjOI9mUH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /project\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-H2WsNB9rk-",
        "outputId": "20b4ecaa-cf3b-4243-d4bc-3392d81b89f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project\n",
            "/project/.gitignore\n",
            "/project/.env\n",
            "/project/utils\n",
            "/project/utils/document_processor.py\n",
            "/project/utils/embedding_manager.py\n",
            "/project/utils/__init__.py\n",
            "/project/utils/retrieval_engine.py\n",
            "/project/utils/llm_interface.py\n",
            "/project/requirements.txt\n",
            "/project/app.py\n",
            "/project/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/requirements.txt\n",
        "# ---------------\n",
        "langchain\n",
        "langchain-community\n",
        "sentence-transformers\n",
        "langchain-huggingface\n",
        "pinecone\n",
        "langchain-pinecone\n",
        "pypdf\n",
        "unstructured\n",
        "streamlit\n",
        "python-dotenv\n",
        "nltk\n",
        "pandas\n",
        "rank_bm25\n",
        "pypdf\n",
        "python-docx\n",
        "pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSF3TlZD9rwe",
        "outputId": "2068a9d0-c855-4b8f-d74b-6bf4e4a94af8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /project/requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDGGMqCg9rzD",
        "outputId": "6d4b0fbf-db87-4868-a871-ea185ae89bdf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 2)) (0.3.23)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 3)) (0.3.21)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 5)) (0.1.2)\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 6)) (6.0.2)\n",
            "Requirement already satisfied: langchain-pinecone in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 7)) (0.2.4)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 8)) (5.4.0)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 9)) (0.17.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 10)) (1.44.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 12)) (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 13)) (2.2.2)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 14)) (0.2.2)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 16)) (1.1.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (from -r /project/requirements.txt (line 17)) (0.11.6)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain->-r /project/requirements.txt (line 2)) (0.3.51)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain->-r /project/requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->-r /project/requirements.txt (line 2)) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain->-r /project/requirements.txt (line 2)) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->-r /project/requirements.txt (line 2)) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain->-r /project/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain->-r /project/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r /project/requirements.txt (line 3)) (3.10.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r /project/requirements.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r /project/requirements.txt (line 3)) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r /project/requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r /project/requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r /project/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /project/requirements.txt (line 4)) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /project/requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /project/requirements.txt (line 4)) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /project/requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /project/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /project/requirements.txt (line 4)) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /project/requirements.txt (line 4)) (11.1.0)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface->-r /project/requirements.txt (line 5)) (0.21.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone->-r /project/requirements.txt (line 6)) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone->-r /project/requirements.txt (line 6)) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone->-r /project/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone->-r /project/requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone->-r /project/requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: langchain-tests<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone->-r /project/requirements.txt (line 7)) (0.3.17)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (5.3.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (4.13.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (2.14.1)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (3.13.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (2.2.1)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (0.32.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (1.17.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured->-r /project/requirements.txt (line 9)) (1.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (18.1.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /project/requirements.txt (line 10)) (6.4.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->-r /project/requirements.txt (line 12)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r /project/requirements.txt (line 12)) (2024.11.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /project/requirements.txt (line 13)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /project/requirements.txt (line 13)) (2025.1)\n",
            "Requirement already satisfied: pdfminer.six==20250327 in /usr/local/lib/python3.11/dist-packages (from pdfplumber->-r /project/requirements.txt (line 17)) (20250327)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber->-r /project/requirements.txt (line 17)) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber->-r /project/requirements.txt (line 17)) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber->-r /project/requirements.txt (line 17)) (43.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r /project/requirements.txt (line 3)) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r /project/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r /project/requirements.txt (line 3)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r /project/requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r /project/requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r /project/requirements.txt (line 3)) (1.18.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r /project/requirements.txt (line 10)) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r /project/requirements.txt (line 10)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r /project/requirements.txt (line 10)) (1.29.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r /project/requirements.txt (line 3)) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r /project/requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r /project/requirements.txt (line 10)) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r /project/requirements.txt (line 4)) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r /project/requirements.txt (line 4)) (2024.10.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain->-r /project/requirements.txt (line 2)) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (8.3.5)\n",
            "Requirement already satisfied: pytest-asyncio<1,>=0.20 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (0.26.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: syrupy<5,>=4 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (4.9.1)\n",
            "Requirement already satisfied: pytest-socket<1,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->-r /project/requirements.txt (line 2)) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->-r /project/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->-r /project/requirements.txt (line 2)) (0.23.0)\n",
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r /project/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r /project/requirements.txt (line 2)) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone->-r /project/requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->-r /project/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r /project/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers->-r /project/requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r /project/requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured->-r /project/requirements.txt (line 9)) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured->-r /project/requirements.txt (line 9)) (0.5.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured->-r /project/requirements.txt (line 9)) (0.47)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->-r /project/requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured->-r /project/requirements.txt (line 9)) (24.1.0)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured->-r /project/requirements.txt (line 9)) (0.2.2)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured->-r /project/requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured->-r /project/requirements.txt (line 9)) (0.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber->-r /project/requirements.txt (line 17)) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r /project/requirements.txt (line 10)) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit->-r /project/requirements.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain->-r /project/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /project/requirements.txt (line 10)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /project/requirements.txt (line 10)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /project/requirements.txt (line 10)) (0.23.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r /project/requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community->-r /project/requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber->-r /project/requirements.txt (line 17)) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone->-r /project/requirements.txt (line 7)) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/.gitignore\n",
        ".env\n",
        "__pycache__/\n",
        "*.py[cod]\n",
        "*$py.class\n",
        ".venv/\n",
        "venv/\n",
        "ENV/\n",
        "env/\n",
        "*.log\n",
        ".DS_Store\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCwqwk419r1-",
        "outputId": "6326631e-910a-4483-87f7-1eaaac9e6be6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/.gitignore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/.env\n",
        "# 🔐 Hugging Face API Key\n",
        "HUGGINGFACE_TOKEN=\n",
        "\n",
        "# 🌲 Pinecone Configuration\n",
        "PINECONE_API_KEY=\n",
        "PINECONE_ENV="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4OZIskTbGvj",
        "outputId": "0560b030-291c-4802-ff19-7d1c62e83565"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/.env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "pinecone_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "pinecone_env = userdata.get(\"PINECONE_ENV\")\n",
        "hf_token = userdata.get(\"HUGGINGFACE_TOKEN\")\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = pinecone_key\n",
        "os.environ[\"PINECONE_ENV\"] = pinecone_env\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = hf_token\n",
        "\n",
        "\n",
        "\n",
        "# Save to .env for your project\n",
        "with open(\"/project/.env\", \"w\") as f:\n",
        "    f.write(f\"PINECONE_API_KEY={pinecone_key}\\n\")\n",
        "    f.write(f\"PINECONE_ENV={pinecone_env}\\n\")\n",
        "    f.write(f\"HUGGINGFACE_TOKEN={hf_token}\\n\")\n"
      ],
      "metadata": {
        "id": "p-17b-iD9r4Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## document_processor.py"
      ],
      "metadata": {
        "id": "AmjqJ56TArIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/utils/document_processor.py\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredFileLoader\n",
        ")\n",
        "from langchain.schema import Document\n",
        "\n",
        "# For enhanced PDF table extraction\n",
        "import pdfplumber\n",
        "# For enhanced DOCX table extraction\n",
        "from docx import Document as DocxDocument\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, chunk_size=1000, chunk_overlap=500, extract_tables=True, table_content_mode=\"markdown\"):\n",
        "        \"\"\"\n",
        "        Initialize DocumentProcessor with options for table handling\n",
        "\n",
        "        Args:\n",
        "            chunk_size: Size of text chunks\n",
        "            chunk_overlap: Overlap between chunks\n",
        "            extract_tables: Whether to extract tables separately\n",
        "            table_content_mode: How to represent tables - options: \"markdown\", \"csv\", \"text\"\n",
        "        \"\"\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        self.extract_tables = extract_tables\n",
        "        self.table_content_mode = table_content_mode\n",
        "\n",
        "    def load_document(self, file_path: str):\n",
        "        \"\"\"Load document based on file extension\"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            logger.error(f\"File not found: {file_path}\")\n",
        "            return []\n",
        "\n",
        "        _, extension = os.path.splitext(file_path)\n",
        "        extension = extension.lower()\n",
        "\n",
        "        try:\n",
        "            documents = []\n",
        "            table_documents = []\n",
        "\n",
        "            # Standard document loading\n",
        "            if extension == '.pdf':\n",
        "                # Use standard loading for text\n",
        "                loader = PyPDFLoader(file_path)\n",
        "                documents = loader.load()\n",
        "\n",
        "                # Extract tables if enabled\n",
        "                if self.extract_tables:\n",
        "                    table_documents = self._extract_tables_from_pdf(file_path)\n",
        "\n",
        "            elif extension == '.docx':\n",
        "                # Use standard loading for text\n",
        "                loader = Docx2txtLoader(file_path)\n",
        "                documents = loader.load()\n",
        "\n",
        "                # Extract tables if enabled\n",
        "                if self.extract_tables:\n",
        "                    table_documents = self._extract_tables_from_docx(file_path)\n",
        "\n",
        "            elif extension == '.txt':\n",
        "                loader = TextLoader(file_path)\n",
        "                documents = loader.load()\n",
        "            else:\n",
        "                loader = UnstructuredFileLoader(file_path, mode=\"elements\")\n",
        "                documents = loader.load()\n",
        "\n",
        "            # Manually add page numbers if not a PDF\n",
        "            if extension != '.pdf':\n",
        "                for i, doc in enumerate(documents):\n",
        "                    if 'page' not in doc.metadata:\n",
        "                        doc.metadata['page'] = i + 1\n",
        "\n",
        "            # Add table documents to regular documents\n",
        "            all_documents = documents + table_documents\n",
        "\n",
        "            return all_documents\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading document {file_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _extract_tables_from_pdf(self, file_path: str) -> List[Document]:\n",
        "        \"\"\"Extract tables from PDF using pdfplumber\"\"\"\n",
        "        table_documents = []\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    tables = page.extract_tables()\n",
        "\n",
        "                    for table_num, table in enumerate(tables):\n",
        "                        if table:\n",
        "                            # Convert table to desired format\n",
        "                            table_content = self._format_table(table)\n",
        "\n",
        "                            # Create document with table content\n",
        "                            table_doc = Document(\n",
        "                                page_content=table_content,\n",
        "                                metadata={\n",
        "                                    'page': page_num + 1,\n",
        "                                    'is_table': True,\n",
        "                                    'table_num': table_num + 1\n",
        "                                }\n",
        "                            )\n",
        "                            table_documents.append(table_doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting tables from PDF {file_path}: {e}\")\n",
        "\n",
        "        return table_documents\n",
        "\n",
        "    def _extract_tables_from_docx(self, file_path: str) -> List[Document]:\n",
        "        \"\"\"Extract tables from DOCX using python-docx\"\"\"\n",
        "        table_documents = []\n",
        "\n",
        "        try:\n",
        "            doc = DocxDocument(file_path)\n",
        "            page_count = 1  # Approximate page count\n",
        "            table_count = 0\n",
        "\n",
        "            for table in doc.tables:\n",
        "                table_count += 1\n",
        "\n",
        "                # Extract data from table\n",
        "                data = []\n",
        "                for i, row in enumerate(table.rows):\n",
        "                    row_data = [cell.text for cell in row.cells]\n",
        "                    data.append(row_data)\n",
        "\n",
        "                # Convert table to desired format\n",
        "                table_content = self._format_table(data)\n",
        "\n",
        "                # Create document with table content\n",
        "                table_doc = Document(\n",
        "                    page_content=table_content,\n",
        "                    metadata={\n",
        "                        'page': page_count,  # Approximate\n",
        "                        'is_table': True,\n",
        "                        'table_num': table_count\n",
        "                    }\n",
        "                )\n",
        "                table_documents.append(table_doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting tables from DOCX {file_path}: {e}\")\n",
        "\n",
        "        return table_documents\n",
        "\n",
        "    def _format_table(self, table_data: List[List[str]]) -> str:\n",
        "        \"\"\"Format table data according to specified mode\"\"\"\n",
        "        if not table_data:\n",
        "            return \"\"\n",
        "\n",
        "        if self.table_content_mode == \"markdown\":\n",
        "            # Convert to markdown table\n",
        "            df = pd.DataFrame(table_data)\n",
        "\n",
        "            # Use first row as header if it looks like a header\n",
        "            if len(set(len(str(x)) for x in table_data[0])) < len(table_data[0]) / 2:\n",
        "                df.columns = df.iloc[0]\n",
        "                df = df.iloc[1:]\n",
        "\n",
        "            return df.to_markdown(index=False)\n",
        "\n",
        "        elif self.table_content_mode == \"csv\":\n",
        "            # Convert to CSV\n",
        "            df = pd.DataFrame(table_data)\n",
        "            csv_buffer = StringIO()\n",
        "            df.to_csv(csv_buffer, index=False)\n",
        "            return csv_buffer.getvalue()\n",
        "\n",
        "        else:  # \"text\" mode\n",
        "            # Convert to plain text\n",
        "            result = []\n",
        "            for row in table_data:\n",
        "                result.append(\" | \".join(str(cell).strip() for cell in row))\n",
        "            return \"\\n\".join(result)\n",
        "\n",
        "    def process_file(self, file_path: str, metadata: Dict[str, Any] = None):\n",
        "        \"\"\"Process file and return chunks with metadata\"\"\"\n",
        "        filename = os.path.basename(file_path)\n",
        "        base_metadata = metadata or {\"filename\": filename}\n",
        "        base_metadata[\"filename\"] = filename  # Ensure filename is always set\n",
        "\n",
        "        documents = self.load_document(file_path)\n",
        "        child_chunks = self.process_documents(documents, base_metadata)\n",
        "\n",
        "        # Create parent chunks with child references\n",
        "        parent_chunks = self.create_parent_chunks(child_chunks)\n",
        "\n",
        "        # Return both parent and child chunks\n",
        "        return child_chunks + parent_chunks\n",
        "\n",
        "    def process_documents(self, documents: List, base_metadata: Dict[str, Any] = None):\n",
        "        \"\"\"Split documents into chunks and create child chunks with metadata\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        # Process table and non-table documents differently\n",
        "        table_chunks = []\n",
        "        non_table_documents = []\n",
        "\n",
        "        for doc in documents:\n",
        "            if doc.metadata.get('is_table', False):\n",
        "                # Don't split table documents - keep them intact\n",
        "                doc.metadata.update({\n",
        "                    \"filename\": base_metadata.get(\"filename\", \"unknown\"),\n",
        "                    \"chunk_id\": f\"table_{doc.metadata.get('page', 1)}_{doc.metadata.get('table_num', 1)}\",\n",
        "                    \"page_no\": str(doc.metadata.get(\"page\", 1)),\n",
        "                    \"is_parent\": \"false\",\n",
        "                    \"is_table\": \"true\",\n",
        "                    \"child_ids\": []\n",
        "                })\n",
        "                table_chunks.append(doc)\n",
        "            else:\n",
        "                non_table_documents.append(doc)\n",
        "\n",
        "        # Process non-table documents normally\n",
        "        chunks = self.text_splitter.split_documents(non_table_documents) if non_table_documents else []\n",
        "        base_metadata = base_metadata or {}\n",
        "\n",
        "        processed_chunks = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            # Create child chunk with required metadata\n",
        "            chunk.metadata = {\n",
        "                \"filename\": base_metadata.get(\"filename\", \"unknown\"),\n",
        "                \"chunk_id\": str(i),  # Convert to string\n",
        "                \"page_no\": str(chunk.metadata.get(\"page\", 1)),  # Convert to string\n",
        "                \"is_parent\": \"false\",  # Convert to string\n",
        "                \"is_table\": \"false\",  # Not a table\n",
        "                \"child_ids\": []  # Empty list for child chunks\n",
        "            }\n",
        "            processed_chunks.append(chunk)\n",
        "\n",
        "        # Combine text chunks and table chunks\n",
        "        return processed_chunks + table_chunks\n",
        "\n",
        "    def create_parent_chunks(self, child_chunks, parent_size=3):\n",
        "        \"\"\"Create parent chunks that reference groups of child chunks\"\"\"\n",
        "        if not child_chunks:\n",
        "            return []\n",
        "\n",
        "        parent_chunks = []\n",
        "\n",
        "        # Group by page for better context\n",
        "        page_groups = {}\n",
        "        for chunk in child_chunks:\n",
        "            page = chunk.metadata[\"page_no\"]\n",
        "            if page not in page_groups:\n",
        "                page_groups[page] = []\n",
        "            page_groups[page].append(chunk)\n",
        "\n",
        "        parent_id = 0\n",
        "        for page, page_chunks in page_groups.items():\n",
        "            # Create parents for each page group\n",
        "            for i in range(0, len(page_chunks), parent_size):\n",
        "                # Determine which child chunks this parent will reference\n",
        "                end_idx = min(i + parent_size, len(page_chunks))\n",
        "                child_ids = [chunk.metadata[\"chunk_id\"] for chunk in page_chunks[i:end_idx]]\n",
        "\n",
        "                # Get texts from child chunks\n",
        "                child_texts = [chunk.page_content for chunk in page_chunks[i:end_idx]]\n",
        "                parent_text = \"\\n\\n\".join(child_texts)\n",
        "\n",
        "                # Create parent metadata\n",
        "                parent_metadata = {\n",
        "                    \"filename\": page_chunks[i].metadata[\"filename\"],\n",
        "                    \"chunk_id\": f\"parent_{parent_id}\",  # Unique parent ID as string\n",
        "                    \"page_no\": str(page),  # Current page\n",
        "                    \"is_parent\": \"true\",  # Convert to string\n",
        "                    \"is_table\": \"false\",  # Not a table\n",
        "                    \"child_ids\": child_ids  # List of string IDs\n",
        "                }\n",
        "\n",
        "                # Create parent document\n",
        "                parent_chunk = Document(\n",
        "                    page_content=parent_text,\n",
        "                    metadata=parent_metadata\n",
        "                )\n",
        "                parent_chunks.append(parent_chunk)\n",
        "                parent_id += 1\n",
        "\n",
        "        return parent_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l31RzjO9r6-",
        "outputId": "98be7d72-d685-43ca-8f06-9646d057b0b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/utils/document_processor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## embedding manager"
      ],
      "metadata": {
        "id": "iroBirfGA0k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/utils/embedding_manager.py\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "from typing import List, Dict, Any\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class EmbeddingManager:\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "        self.dimension = 768  # Embedding dimension for this model\n",
        "        self.index_name = \"document-chatbot\"\n",
        "        self.pinecone_initialized = False\n",
        "        self.vectorstore = None\n",
        "\n",
        "    def init_pinecone(self):\n",
        "        \"\"\"Initialize Pinecone connection\"\"\"\n",
        "        api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "        cloud = os.getenv(\"PINECONE_CLOUD\", \"aws\")\n",
        "        region = os.getenv(\"PINECONE_REGION\", \"us-east-1\")\n",
        "\n",
        "        if not api_key:\n",
        "            logger.warning(\"Missing Pinecone API key in environment variables.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Create Pinecone instance\n",
        "            pc = Pinecone(api_key=api_key)\n",
        "            logger.info(\"Pinecone client initialized.\")\n",
        "\n",
        "            # Check if index exists and create if needed\n",
        "            if self.index_name not in pc.list_indexes().names():\n",
        "                logger.info(f\"Creating Pinecone index '{self.index_name}'...\")\n",
        "\n",
        "                # Create the index with ServerlessSpec\n",
        "                pc.create_index(\n",
        "                    name=self.index_name,\n",
        "                    dimension=self.dimension,\n",
        "                    metric=\"dotproduct\",\n",
        "                    spec=ServerlessSpec(cloud=cloud, region=region)\n",
        "                )\n",
        "\n",
        "                # Wait for index to be ready\n",
        "                logger.info(\"Waiting for index to be ready...\")\n",
        "                while not pc.describe_index(self.index_name).status['ready']:\n",
        "                    time.sleep(1)\n",
        "\n",
        "                logger.info(f\"Pinecone index '{self.index_name}' is ready.\")\n",
        "            else:\n",
        "                logger.info(f\"Using existing Pinecone index '{self.index_name}'.\")\n",
        "\n",
        "            self.pinecone_initialized = True\n",
        "            return pc\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing Pinecone: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_vectorstore(self, use_pinecone=True):\n",
        "        \"\"\"Get or create vector store\"\"\"\n",
        "        if use_pinecone:\n",
        "            pc = self.init_pinecone()\n",
        "            if pc:\n",
        "                # Create the PineconeVectorStore using the updated import\n",
        "                self.vectorstore = PineconeVectorStore(\n",
        "                    index_name=self.index_name,\n",
        "                    embedding=self.embeddings,\n",
        "                    text_key=\"text\"\n",
        "                )\n",
        "                logger.info(\"Using Pinecone vector store.\")\n",
        "                return self.vectorstore\n",
        "\n",
        "        # Fallback to FAISS if Pinecone fails or isn't requested\n",
        "        logger.warning(\"Falling back to local FAISS vector store.\")\n",
        "        self.vectorstore = FAISS.from_texts(\n",
        "            texts=[\"Initial document\"],\n",
        "            embedding=self.embeddings,\n",
        "            metadatas=[{\n",
        "                \"filename\": \"initialization\",\n",
        "                \"chunk_id\": \"init_0\",\n",
        "                \"page_no\": \"1\",\n",
        "                \"is_parent\": \"false\",\n",
        "                \"child_ids\": []\n",
        "            }]\n",
        "        )\n",
        "        logger.info(\"Created empty FAISS vector store.\")\n",
        "        return self.vectorstore\n",
        "\n",
        "    def load_local_vectorstore(self, path: str):\n",
        "        \"\"\"Load FAISS vector store from disk\"\"\"\n",
        "        try:\n",
        "            with open(path, 'rb') as f:\n",
        "                self.vectorstore = pickle.load(f)\n",
        "            logger.info(f\"Loaded local FAISS vector store from {path}\")\n",
        "            return self.vectorstore\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading vector store: {e}\")\n",
        "            return None\n",
        "\n",
        "    def save_local_vectorstore(self, path: str):\n",
        "        \"\"\"Save FAISS vector store to disk\"\"\"\n",
        "        if not self.vectorstore or not isinstance(self.vectorstore, FAISS):\n",
        "            logger.warning(\"No FAISS vector store available to save.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            with open(path, 'wb') as f:\n",
        "                pickle.dump(self.vectorstore, f)\n",
        "            logger.info(f\"Saved FAISS vector store to {path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving vector store: {e}\")\n",
        "            return False\n",
        "\n",
        "    def index_documents(self, documents: List[Document]):\n",
        "        \"\"\"Index documents into vector store\"\"\"\n",
        "        if not documents:\n",
        "            logger.warning(\"No documents provided for indexing.\")\n",
        "            return False\n",
        "\n",
        "        if not self.vectorstore:\n",
        "            self.get_vectorstore()\n",
        "\n",
        "        try:\n",
        "            # Ensure only required metadata fields are present with proper types\n",
        "            for doc in documents:\n",
        "                # Handle page values from different sources\n",
        "                page = doc.metadata.get(\"page_no\", doc.metadata.get(\"page\", 1))\n",
        "                if not isinstance(page, str):\n",
        "                    page = str(page)\n",
        "\n",
        "                # Handle chunk_id\n",
        "                chunk_id = doc.metadata.get(\"chunk_id\", \"0\")\n",
        "                if not isinstance(chunk_id, str):\n",
        "                    chunk_id = str(chunk_id)\n",
        "\n",
        "                # Handle is_parent\n",
        "                is_parent = doc.metadata.get(\"is_parent\", \"false\")\n",
        "                if not isinstance(is_parent, str):\n",
        "                    is_parent = \"true\" if is_parent else \"false\"\n",
        "\n",
        "                # Handle child_ids, ensuring it's a list of strings\n",
        "                child_ids = doc.metadata.get(\"child_ids\", [])\n",
        "                if child_ids:\n",
        "                    child_ids = [str(child_id) for child_id in child_ids]\n",
        "\n",
        "                # Keep only the required fields with proper types\n",
        "                doc.metadata = {\n",
        "                    \"filename\": str(doc.metadata.get(\"filename\", \"unknown\")),\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"page_no\": page,\n",
        "                    \"is_parent\": is_parent,\n",
        "                    \"child_ids\": child_ids  # List of strings\n",
        "                }\n",
        "\n",
        "            texts = [doc.page_content for doc in documents]\n",
        "            metadatas = [doc.metadata for doc in documents]\n",
        "            self.vectorstore.add_texts(texts, metadatas)\n",
        "            logger.info(f\"Indexed {len(documents)} documents.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error indexing documents: {e}\")\n",
        "            return False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j66ySMJm9r9W",
        "outputId": "6eeb188e-8ebf-4f0b-d554-294c38467053"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/utils/embedding_manager.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## retrieval_engine"
      ],
      "metadata": {
        "id": "xh7NgkiIA5n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/utils/retrieval_engine.py\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RetrievalEngine:\n",
        "    def __init__(self, vectorstore):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.vector_retriever = None\n",
        "        self.bm25_retriever = None\n",
        "        self.ensemble_retriever = None\n",
        "\n",
        "    def setup_vector_retriever(self, k: int = 4, filter_dict: Dict[str, Any] = None):\n",
        "        \"\"\"Set up vector retriever with optional metadata filtering.\"\"\"\n",
        "        search_kwargs = {\"k\": k}\n",
        "        if filter_dict:\n",
        "            search_kwargs[\"filter\"] = filter_dict\n",
        "\n",
        "        self.vector_retriever = self.vectorstore.as_retriever(search_kwargs=search_kwargs)\n",
        "        logger.info(f\"Vector retriever set up with top-k={k}, filter={filter_dict}\")\n",
        "        return self.vector_retriever\n",
        "\n",
        "    def setup_bm25_retriever(self, documents: List, k: int = 4):\n",
        "        \"\"\"Set up BM25 sparse retriever.\"\"\"\n",
        "        self.bm25_retriever = BM25Retriever.from_documents(documents, k=k)\n",
        "        logger.info(f\"BM25 retriever set up with top-k={k}\")\n",
        "        return self.bm25_retriever\n",
        "\n",
        "    def setup_ensemble_retriever(self, weights: List[float] = None):\n",
        "        \"\"\"Set up ensemble retriever combining vector and BM25.\"\"\"\n",
        "        if not self.vector_retriever or not self.bm25_retriever:\n",
        "            raise ValueError(\"Both vector and BM25 retrievers must be initialized before ensemble.\")\n",
        "\n",
        "        weights = weights or [0.7, 0.3]\n",
        "        self.ensemble_retriever = EnsembleRetriever(\n",
        "            retrievers=[self.vector_retriever, self.bm25_retriever],\n",
        "            weights=weights\n",
        "        )\n",
        "        logger.info(f\"Ensemble retriever set up with weights={weights}\")\n",
        "        return self.ensemble_retriever\n",
        "\n",
        "    def get_retriever(self, retriever_type: str = \"vector\"):\n",
        "        \"\"\"Return the appropriate retriever instance.\"\"\"\n",
        "        if retriever_type == \"vector\" and self.vector_retriever:\n",
        "            return self.vector_retriever\n",
        "        elif retriever_type == \"bm25\" and self.bm25_retriever:\n",
        "            return self.bm25_retriever\n",
        "        elif retriever_type == \"ensemble\" and self.ensemble_retriever:\n",
        "            return self.ensemble_retriever\n",
        "        else:\n",
        "            logger.warning(f\"Unknown or uninitialized retriever type '{retriever_type}', falling back to vector retriever.\")\n",
        "            if not self.vector_retriever:\n",
        "                self.setup_vector_retriever()\n",
        "            return self.vector_retriever\n",
        "\n",
        "    def retrieve(self, query: str, retriever_type: str = \"vector\", k: int = 4, filter_dict: Dict[str, Any] = None):\n",
        "        \"\"\"Retrieve relevant documents for a query.\"\"\"\n",
        "        try:\n",
        "            if retriever_type == \"vector\":\n",
        "                self.setup_vector_retriever(k=k, filter_dict=filter_dict)\n",
        "                retriever = self.vector_retriever\n",
        "            else:\n",
        "                retriever = self.get_retriever(retriever_type)\n",
        "\n",
        "            results = retriever.get_relevant_documents(query)\n",
        "            logger.info(f\"Retrieved {len(results)} documents using '{retriever_type}' retriever.\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during retrieval: {e}\")\n",
        "            return []\n",
        "\n",
        "    def retrieve_with_score(self, query: str, k: int = 4):\n",
        "        \"\"\"Retrieve documents with similarity scores from vector store.\"\"\"\n",
        "        try:\n",
        "            results = self.vectorstore.similarity_search_with_score(query, k=k)\n",
        "            logger.info(f\"Retrieved {len(results)} scored documents for query: '{query}'\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error retrieving with scores: {e}\")\n",
        "            return []\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY9o07La9sAP",
        "outputId": "0e7819c5-923e-4708-d638-d1c62be74d63"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/utils/retrieval_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM interface"
      ],
      "metadata": {
        "id": "psU6cSEdBAf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/utils/llm_interface.py\n",
        "import os\n",
        "import logging\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LLMInterface:\n",
        "    def __init__(self, retriever=None):\n",
        "        self.llm = None\n",
        "        self.retriever = retriever\n",
        "        self.qa_chain = None\n",
        "\n",
        "    def setup_llm(self, model_id: str = \"mistralai/Mistral-7B-Instruct-v0.2\", temperature: float = 0.5):\n",
        "        try:\n",
        "            self.llm = HuggingFaceEndpoint(\n",
        "                repo_id=model_id,\n",
        "                huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
        "                temperature=temperature,\n",
        "                task=\"text-generation\"\n",
        "            )\n",
        "            logger.info(f\"LLM '{model_id}' initialized with temperature={temperature}\")\n",
        "            return self.llm\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error setting up LLM: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_qa_chain(self, chain_type: str = \"stuff\", return_source_docs: bool = True):\n",
        "        try:\n",
        "            if not self.llm:\n",
        "                self.setup_llm()\n",
        "            if not self.retriever:\n",
        "                raise ValueError(\"Retriever must be set before creating QA chain\")\n",
        "\n",
        "            qa_template = \"\"\"\n",
        "You are a helpful assistant that answers questions based only on the context provided.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer Guidelines:\n",
        "- IMPORTANT: Use well-structured tables to present information when appropriate, especially for:\n",
        "  * Comparing multiple items, features, or options\n",
        "  * Presenting data with multiple attributes or columns\n",
        "  * Showing structured information like specifications, statistics, or schedules\n",
        "- Use Markdown tables with proper alignment for all tabular data\n",
        "- Use bullet points for lists and enumerations\n",
        "- Write clear and structured paragraphs for explanations and narratives\n",
        "- Be concise, but ensure completeness\n",
        "- For numerical or categorical information, always consider if a table would be clearer than text\n",
        "- If unsure or if the answer is not found in the context, say \"I don't have enough information to answer this question.\"\n",
        "\n",
        "Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "            prompt = PromptTemplate.from_template(qa_template)\n",
        "\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,\n",
        "                chain_type=chain_type,\n",
        "                retriever=self.retriever,\n",
        "                return_source_documents=return_source_docs,\n",
        "                chain_type_kwargs={\"prompt\": prompt}\n",
        "            )\n",
        "\n",
        "            logger.info(\"QA chain successfully created.\")\n",
        "            return self.qa_chain\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating QA chain: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process_query(self, query: str):\n",
        "        if not self.qa_chain:\n",
        "            if not self.retriever:\n",
        "                raise ValueError(\"Retriever must be set before processing queries\")\n",
        "            self.create_qa_chain()\n",
        "\n",
        "        try:\n",
        "            response = self.qa_chain.invoke({\"query\": query})\n",
        "            return {\n",
        "                \"answer\": response[\"result\"],\n",
        "                \"source_documents\": response.get(\"source_documents\", [])\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {e}\")\n",
        "            return {\n",
        "                \"answer\": f\"Error processing your query: {str(e)}\",\n",
        "                \"source_documents\": []\n",
        "            }\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gwQzZjD9sC8",
        "outputId": "7ce0e18e-6bab-4a2d-f27e-42418bd855f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/utils/llm_interface.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main.py"
      ],
      "metadata": {
        "id": "2Nki9GP4B-Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/main.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from utils.document_processor import DocumentProcessor\n",
        "from utils.embedding_manager import EmbeddingManager\n",
        "from utils.retrieval_engine import RetrievalEngine\n",
        "from utils.llm_interface import LLMInterface\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "class DocumentChatbot:\n",
        "    def __init__(self):\n",
        "        self.document_processor = DocumentProcessor(chunk_size=500, chunk_overlap=100)\n",
        "        self.embedding_manager = EmbeddingManager()\n",
        "        self.vectorstore = None\n",
        "        self.retrieval_engine = None\n",
        "        self.llm_interface = None\n",
        "        self.documents = []\n",
        "\n",
        "    def load_and_process_document(self, file_path, metadata=None):\n",
        "        print(f\"Processing document: {file_path}\")\n",
        "        chunks = self.document_processor.process_file(file_path, metadata)\n",
        "        self.documents.extend(chunks)\n",
        "        print(f\"Document processed into {len(chunks)} chunks\")\n",
        "        return chunks\n",
        "\n",
        "    def setup_vectorstore(self, use_pinecone=True):\n",
        "        self.vectorstore = self.embedding_manager.get_vectorstore(use_pinecone)\n",
        "        return self.vectorstore\n",
        "\n",
        "    def index_documents(self):\n",
        "        if not self.vectorstore:\n",
        "            self.setup_vectorstore()\n",
        "        print(f\"Indexing {len(self.documents)} document chunks\")\n",
        "        result = self.embedding_manager.index_documents(self.documents)\n",
        "        if result:\n",
        "            print(\"Documents indexed successfully\")\n",
        "        else:\n",
        "            print(\"Failed to index documents\")\n",
        "        return result\n",
        "\n",
        "    def setup_retrieval(self):\n",
        "        if not self.vectorstore:\n",
        "            self.setup_vectorstore()\n",
        "        self.retrieval_engine = RetrievalEngine(self.vectorstore)\n",
        "        self.retrieval_engine.setup_vector_retriever(k=4)\n",
        "        if self.documents:\n",
        "            self.retrieval_engine.setup_bm25_retriever(self.documents, k=4)\n",
        "            self.retrieval_engine.setup_ensemble_retriever(weights=[0.7, 0.3])\n",
        "        return self.retrieval_engine\n",
        "\n",
        "    def setup_llm(self):\n",
        "        if not self.retrieval_engine:\n",
        "            self.setup_retrieval()\n",
        "        retriever = self.retrieval_engine.get_retriever(\"ensemble\")\n",
        "        self.llm_interface = LLMInterface(retriever)\n",
        "        self.llm_interface.setup_llm()\n",
        "        self.llm_interface.create_qa_chain()\n",
        "        return self.llm_interface\n",
        "\n",
        "    def ask(self, query):\n",
        "        if not self.llm_interface:\n",
        "            self.setup_llm()\n",
        "        return self.llm_interface.process_query(query)\n",
        "\n",
        "    def save_vectorstore(self, path=\"vectorstore.pkl\"):\n",
        "        if not self.embedding_manager.vectorstore:\n",
        "            print(\"No vector store to save\")\n",
        "            return False\n",
        "        return self.embedding_manager.save_local_vectorstore(path)\n",
        "\n",
        "    def load_vectorstore(self, path=\"vectorstore.pkl\"):\n",
        "        self.vectorstore = self.embedding_manager.load_local_vectorstore(path)\n",
        "        return self.vectorstore is not None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4INGZ6e9sFG",
        "outputId": "fd85f04e-ab30-4ad4-9bd5-dcaebe9efaca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /project\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-FMq5W6D_H5",
        "outputId": "5d31e930-cf98-4d94-fb09-0a1ff84fe675"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tmain.py  requirements.txt  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /project/utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ6XyS8FEEgs",
        "outputId": "8b38f33d-58c6-4b88-b4fb-1aa1ddfa50f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document_processor.py  embedding_manager.py  __init__.py  llm_interface.py  retrieval_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/project')"
      ],
      "metadata": {
        "id": "nyGhy35F9sH4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /project/app.py\n",
        "# /project/app.py\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "from main import DocumentChatbot\n",
        "\n",
        "# Session state initialization\n",
        "if 'chatbot' not in st.session_state:\n",
        "    st.session_state.chatbot = DocumentChatbot()\n",
        "\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "if 'documents_processed' not in st.session_state:\n",
        "    st.session_state.documents_processed = False\n",
        "\n",
        "def add_message(role, content):\n",
        "    \"\"\"Add message to chat history\"\"\"\n",
        "    st.session_state.messages.append({\"role\": role, \"content\": content})\n",
        "\n",
        "def save_uploaded_file(uploaded_file):\n",
        "    \"\"\"Save uploaded file to a temporary location\"\"\"\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:\n",
        "        tmp.write(uploaded_file.getvalue())\n",
        "        return tmp.name\n",
        "\n",
        "def highlight_text(text, search_terms):\n",
        "    \"\"\"Highlight search terms in the given text\"\"\"\n",
        "    if not search_terms:\n",
        "        return text\n",
        "    for term in search_terms:\n",
        "        text = text.replace(term, f\"**{term}**\")\n",
        "    return text\n",
        "\n",
        "# App layout\n",
        "st.set_page_config(page_title=\"📄 Document Chatbot\", layout=\"wide\")\n",
        "st.title(\"📄💬 Document Chatbot\")\n",
        "st.markdown(\"Upload documents and ask questions. The bot will answer based on the content and show sources!\")\n",
        "\n",
        "# Sidebar: Upload and Vector Store\n",
        "with st.sidebar:\n",
        "    st.header(\"📤 Document Upload & Processing\")\n",
        "\n",
        "    uploaded_files = st.file_uploader(\"Upload documents\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
        "\n",
        "    if uploaded_files and st.button(\"🚀 Process Documents\"):\n",
        "        with st.spinner(\"Processing and indexing documents...\"):\n",
        "            for uploaded_file in uploaded_files:\n",
        "                file_path = save_uploaded_file(uploaded_file)\n",
        "                st.session_state.chatbot.load_and_process_document(file_path, {\"source\": uploaded_file.name})\n",
        "\n",
        "            if st.session_state.chatbot.index_documents():\n",
        "                st.session_state.documents_processed = True\n",
        "                st.session_state.chatbot.setup_retrieval()\n",
        "                st.session_state.chatbot.setup_llm()\n",
        "                st.success(f\"✅ Successfully processed {len(uploaded_files)} documents\")\n",
        "            else:\n",
        "                st.error(\"❌ Failed to index documents.\")\n",
        "\n",
        "    st.header(\"💾 Vector Store Management\")\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        if st.button(\"💽 Save Vector Store\"):\n",
        "            if st.session_state.chatbot.save_vectorstore():\n",
        "                st.success(\"✅ Vector store saved.\")\n",
        "            else:\n",
        "                st.error(\"❌ Failed to save vector store.\")\n",
        "\n",
        "    with col2:\n",
        "        if st.button(\"📥 Load Vector Store\"):\n",
        "            if st.session_state.chatbot.load_vectorstore():\n",
        "                st.session_state.documents_processed = True\n",
        "                st.session_state.chatbot.setup_retrieval()\n",
        "                st.session_state.chatbot.setup_llm()\n",
        "                st.success(\"✅ Vector store loaded.\")\n",
        "            else:\n",
        "                st.error(\"❌ Failed to load vector store.\")\n",
        "\n",
        "# Chat history\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "        if message[\"role\"] == \"assistant\" and \"sources\" in message:\n",
        "            with st.expander(\"🔍 View Sources\"):\n",
        "                for i, source in enumerate(message[\"sources\"]):\n",
        "                    st.markdown(f\"**Source {i+1}:** {source['metadata'].get('source', 'Unknown')}\")\n",
        "                    st.markdown(highlight_text(source[\"content\"], [q.strip() for q in message.get(\"query\", \"\").split()]))\n",
        "\n",
        "# Chat input\n",
        "query = st.chat_input(\"💬 Ask a question about your documents\")\n",
        "if query:\n",
        "    # Display user query\n",
        "    add_message(\"user\", query)\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(query)\n",
        "\n",
        "    # Answer from assistant\n",
        "    if not st.session_state.documents_processed:\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.write(\"⚠️ Please upload and process documents first.\")\n",
        "        add_message(\"assistant\", \"⚠️ Please upload and process documents first.\")\n",
        "    else:\n",
        "        with st.spinner(\"🤖 Thinking...\"):\n",
        "            response = st.session_state.chatbot.ask(query)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.write(response[\"answer\"])\n",
        "            sources = []\n",
        "            if response.get(\"source_documents\"):\n",
        "                with st.expander(\"🔍 View Sources\"):\n",
        "                    for i, doc in enumerate(response[\"source_documents\"]):\n",
        "                        st.markdown(f\"**Source {i+1}:** {doc.metadata.get('source', 'Unknown')}\")\n",
        "                        st.markdown(highlight_text(doc.page_content, [q.strip() for q in query.split()]))\n",
        "                        sources.append({\"metadata\": doc.metadata, \"content\": doc.page_content})\n",
        "\n",
        "        add_message(\"assistant\", response[\"answer\"])\n",
        "        st.session_state.messages[-1][\"query\"] = query\n",
        "        st.session_state.messages[-1][\"sources\"] = sources if sources else []\n"
      ],
      "metadata": {
        "id": "yD15JA4B-Qaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b2be86-dcca-438a-961e-aa2a7b8e6ded"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /project/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok\n"
      ],
      "metadata": {
        "id": "9KZCOY8M-p7D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"")\n"
      ],
      "metadata": {
        "id": "Ns8N0JsCAOh6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import os\n",
        "\n",
        "def run_app():\n",
        "    os.system(\"streamlit run /project/app.py\")\n",
        "\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "dBIv_oW-AQO6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(addr=\"8501\")\n",
        "print(f\"🌐 Your Streamlit app is live at: {public_url}\")\n"
      ],
      "metadata": {
        "id": "OMgH9MXK-m1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b827974-9014-47aa-fade-0d28cdac6061"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Your Streamlit app is live at: NgrokTunnel: \"https://ecdd-34-168-160-58.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7gaYmaQCA_Xi"
      }
    }
  ]
}
